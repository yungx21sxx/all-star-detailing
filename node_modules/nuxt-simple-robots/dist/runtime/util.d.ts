import type { NitroRouteConfig } from 'nitropack';
import type { ParsedRobotsTxt, RobotsGroupInput, RobotsGroupResolved } from './types.js';
/**
 * We're going to read the robots.txt and extract any disallow or sitemaps rules from it.
 *
 * We're going to use the Google specification, the keys should be checked:
 *
 * - user-agent: identifies which crawler the rules apply to.
 * - allow: a URL path that may be crawled.
 * - disallow: a URL path that may not be crawled.
 * - sitemap: the complete URL of a sitemap.
 * - host: the host name of the site, this is optional non-standard directive.
 *
 * @see https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt
 */
export declare function parseRobotsTxt(s: string): ParsedRobotsTxt;
export declare function validateRobots(robotsTxt: ParsedRobotsTxt): string[];
export declare function asArray(v: any): any[];
export declare function indexableFromGroup(groups: RobotsGroupInput[], path: string): boolean;
export declare function generateRobotsTxt({ groups, sitemaps }: {
    groups: RobotsGroupResolved[];
    sitemaps: string[];
}): string;
export declare function mergeOnKey<T, K extends keyof T>(arr: T[], key: K): T[];
export declare function isInternalRoute(_path: string): boolean;
export declare function normaliseRobotsRouteRule(rules: NitroRouteConfig, defaultIndexable: boolean, disabledValue: string, enabledValue: string): {
    indexable: any;
    rule: string;
};
